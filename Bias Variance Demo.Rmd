---
title: "Bias Variance Trade off"
author: "Christopher Howard"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(data.table)
```

## Introduction
In this project, we will explore one of the most fundamental ideas to Machine Learning: the bias-variance trade-off. This is the idea that as we decrease the complexity of a model (ie. fitting the target function with a simpler model), the bias will increase and variance will decrease. Similarly, as models become more complex, bias will decrease and variance will increase. For some arbitrary observation $x_0$ and a model $f'$, we can decompose its expected MSE as a function of bias squared and variance:
$$E(y_0 - f'(x_0))=Var(f'(x_0))+[Bias(f'(x_0))]^2 + Var(\epsilon) $$

If we want to minimize test MSE, it is sufficient to minimize variance and bias (James et al., 2021). Test MSE tends to start high, then decrease, then increase as model flexibility increases. In practice it is difficult to find a model with low variance and low bias. 


## Simulating Data for Exploration

Here we will generate a random predictor of length n=200 and an n=200 length error vector. We will also create a response vector $Y$ of length n=200 according to the model $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$$ For this project $\beta_n$ can be any real constant, but I choose $\beta_0 = 1$ $\beta_1 = 2$, $\beta_2 = 3$. We also do a simple 75/25 train-test split.

```{r}
set.seed(2401)
X <- rnorm(200)
e <- rnorm(200)
Y= 1 + 2*X + 3*X^2 + e
df <- data.frame(Y,X)

training.samples <- df$X %>% 
  createDataPartition(p=.75) %>% unlist()
test.data<- df[-training.samples,]
train.data = df[training.samples,]

```


Now we want to fit the following seven models to the simulated data. 

$$
Y = \beta_0 + \beta_1 X
$$ 
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2
$$ 
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3
$$
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 + \beta_5 X^5
$$
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3...+\beta_{10} X^{10}
$$
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3...+\beta_{15} X^{15}
$$
$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3...+\beta_{20} X^{20}
$$



```{r}
mod1<- lm(Y~X, data = train.data)
mod2 <- lm(Y~poly(X,2), data=train.data)
mod3 <- lm(Y~poly(X,3), data=train.data)
mod5 <- lm(Y~poly(X,5), data=train.data)
mod10 <- lm(Y~poly(X,10), data=train.data)
mod15 <- lm(Y~poly(X,15), data=train.data)
mod20 <- lm(Y~poly(X,20), data=train.data)

models <- list(mod1, mod2, mod3, mod5, mod10, mod15, mod20)
```

Now we plot the models to the simulated data.
```{r}

par(mfrow=c(3,3))
predictor_degrees <- c(1,2,3,5,10,15,20)
j=1

for (i in models){
  plot(Y ~ X, data = df, col = "grey", pch = 0, main = paste(predictor_degrees[j], "Degree"))
    lines(sort(train.data$X), fitted(i)[order(train.data$X)], col = "red")
j=j+1
}
```
  
  From these plots it is a little hard to gauge which fit does the best. The one degree model appears to fit the data the weakest. The ground truth is quadratic, so it is not surprising that the polynomial fits appear to better fit the data, as they have the complexity to represent non-linear relationships.

## Getting Data Metrics

  Now we will look at a few metrics for the models. We will get the test RMSE for each model. We will also randomly select five points from the test data set, and compute the bias squared and variance for those five data points. We use bias squared to guarantee a positive value for bias to better compare models. 
```{r}
comp_list<- list()
for (i in models){
  predictions<-i %>% predict(test.data)
  compare<- data.frame(actual=test.data$Y,
                       predicted= predictions)
  comp_list<- append(comp_list, compare)
}


mod1predicted<-unlist(comp_list[2])
mod2predicted<-unlist(comp_list[4])
mod3predicted<-unlist(comp_list[6])
mod5predicted<-unlist(comp_list[8])
mod10predicted<-unlist(comp_list[10])
mod15predicted<-unlist(comp_list[12])
mod20predicted<-unlist(comp_list[14])

test.predicted<-list(mod1predicted, mod2predicted, mod3predicted, mod5predicted, mod10predicted, mod15predicted, mod20predicted)

error_vector<- c() 
for (i in test.predicted){
  error<-RMSE(unname(i), test.data$Y)
  error_vector <-append(error_vector, error)
}
labels <- c("Degree 1 Fit","Degree 2 Fit", "Degree 3 Fit","Degree 5 Fit", "Degree 10 Fit", "Degree 15 Fit", "Degree 20 Fit")

modassess.df<- data.frame(labels, error_vector)

library(SimDesign)

set.seed(12345)
rand_ints<- sample(1:48,5)
x0<-test.data$X[rand_ints]
fx0<-test.data$Y[rand_ints]

p<- data.frame(X=c(x0))


predmod1<-predict(mod1, newdata= p)
predmod2<-predict(mod2, newdata= p)
predmod3<-predict(mod3, newdata= p)
predmod5<-predict(mod5, newdata= p)
predmod10<-predict(mod10, newdata= p)
predmod15<-predict(mod15, newdata= p)
predmod20<-predict(mod20, newdata= p)
predx0 <- list(predmod1, predmod2, predmod3, predmod5, predmod10, predmod15, predmod20)

biaslist<- c()
for (i in predx0){
  b<- ((SimDesign::bias(fx0,i))^2)
  biaslist<-append(biaslist, b) 
}
biaslist<-biaslist %>% rev()

varlist<- c()
for (i in predx0){
  varlist<- var(i) %>% append(varlist)
}
varlist<-varlist %>%  rev()

modassess.df$B<- biaslist
modassess.df$Variance<- varlist
colnames(modassess.df)[1]<-"Model"
colnames(modassess.df)[2]<-"RMSE"
colnames(modassess.df)[3]<- "Bias Squared"
modassess.tb<- setDT(modassess.df)
print(modassess.tb)
```
## Assessing the Models 

  There is a lot to unpack here. Let's look at bias first. Bias generally decreases with model complexity, which is to be expected. For the one degree model, the model is not complex enough to pick up on the ground truth, so our predictions are consistently incorrect. Thus we have high bias. Conversely, the degree 20 polynomial model is complex so we are making more assumptions about the form of the ground truth. Thus we have high  bias.

  Variance generally increases as model complexity increases, which we also expected. The 20 degree polynomial model follows the training data very well, but adding a new observation can throw off the model's estimate, so variance is high. The one degree model on the other hand can handle the addition of a new observation without changing its slope much, so variance is low. Model bias and variance appears inversely proportional.

  The polynomial degree 2 model has the lowest test RMSE, but the degree 3 polynomial model also has very low RMSE. Since the ground truth is a degree 2 polynomial, it makes sense that the degree 2 model and degree 3 models are the most successful at predicting in terms of RMSE. They both have the complexity to match the target function with less risk of overfitting in comparison to the more complex models. The degree 15 and 20 polynomial models have very high test error, indicting overfitting.
In general the RMSE starts high, then decreases, then increases with model complexity. This makes sense, as MSE (and subsequently RMSE) is a function of variance and bias. To minimize test error, we want low bias and low variance. The polynomial degree 2 model has relatively low bias and variance and is the best model according to RMSE. This exemplifies the bias-variance tradeoff- a low bias and low variance model will have low MSE. It's just a matter of finding a model with such qualities!



## Works Cited
James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning with applications in R. Springer. 
